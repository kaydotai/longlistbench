Existing datasets for evaluating document extraction using large language models (LLMs) predominantly focus on key-value pair extraction and fail to address the complexities of long-list entity extraction. However, semi-structured business documents with long incident/line-item lists---such as invoices, purchase orders, and claims summaries---commonly organize data in table-like structures with dozens to hundreds of repetitive entities of the same type, creating a critical gap in current benchmarking efforts. We introduce LongListBench, a synthetic benchmark for long-list entity extraction in semi-structured business documents, inspired by recurring patterns observed in real-world claims documents. Each instance pairs structured ground truth (JSON) with a rendered PDF and an OCR transcript produced by a vision-language model, enabling reproducible evaluation of end-to-end pipelines under layout and transcription noise. We systematically inject seven document phenomena observed in production settings, including page breaks, multi-row entities, exact duplicates, large documents, multiple tables, multi-column layout, and merged cells. Across 80 documents (6{,}828 incident rows), identifier coverage in OCR is near-perfect, while schema-conformant zero-shot LLM baselines achieve 89.1\%, 82.9\%, and 86.5\% average field-level F1 for GPT-4o, GPT-5.2, and Gemini 2.0 Flash, respectively, on a hard+extreme evaluation subset. LongListBench targets the measurement problem; algorithmic solutions for robust long-list extraction are left to future work.