Existing datasets for evaluating document extraction using large language models (LLMs) predominantly focus on key-value pair extraction and fail to address the complexities of long-list entity extraction. However, real-world business documents such as invoices, loss runs, purchase orders, and financial statements commonly organize data in table-like structures with dozens to hundreds of repetitive entities of the same type---a critical gap in current benchmarking efforts. We introduce Lost-and-Found Entities, a synthetic benchmark for long-list extraction in trucking insurance loss run documents. Each instance pairs structured ground truth (JSON) with a rendered PDF and an OCR transcript produced by a vision-language model, enabling reproducible evaluation of end-to-end pipelines under realistic transcription noise. We systematically inject seven document phenomena observed in production settings, including page breaks, multi-row entities, exact duplicates, large documents, multiple tables, multi-column layout, and merged cells. Across 80 documents (6{,}828 incident rows), identifier coverage in OCR is near-perfect, while zero-shot LLM baselines (GPT-4o and Claude Sonnet 4) achieve 96.3\% incident-level F1 on five detailed-format documents under a shared prompt. Our benchmark targets the measurement problem; algorithmic solutions for robust long-list extraction are left to future work.

% TODO:
% Make examples more robust: invoices, purchase orders, and financial statements are pretty much the same
% Add note about using Redacto for evals (Can switch to extend instead, Achyut mentioned he talked to their CEO before, might be a better connection for publicity)