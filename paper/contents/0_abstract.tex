Existing datasets for evaluating document extraction using large language models (LLMs) predominantly focus on key-value pair extraction and fail to address the complexities of long list entity extraction. However, real-world business documents such as invoices, insurance claims, purchase orders, and financial statements commonly organize data in table-like structures with repetitive entities of the same type---a critical gap in current benchmarking efforts. We introduce a comprehensive dataset specifically designed to evaluate long list extraction performance, incorporating common challenges observed in production environments, including true duplicate entities, multi-row entries, and various structural inconsistencies. Our dataset construction methodology leverages a corpus of real-world insurance claims gathered through Kay.ai operations, combined with systematically identified problematic patterns in table-like documents. We employ LLMs to generate realistic document layouts, which are subsequently rendered into PDFs and processed through optical character recognition (OCR) to simulate authentic extraction scenarios complete with real-world noise. Additionally, we provide standardized evaluation scripts to facilitate reproducible assessments. We benchmark flagship models from OpenAI, Anthropic, and Google using practical, easily implementable techniques including zero-shot prompting and accumulative generation. Our work addresses a significant gap in document understanding evaluation and provides the research community with essential tools for advancing long list extraction capabilities.

% TODO:
% Make examples more robust: invoices, purchase orders, and financial statements are pretty much the same
% Add note about using Redacto for evals (Can switch to extend instead, Achyut mentioned he talked to their CEO before, might be a better connection for publicity)