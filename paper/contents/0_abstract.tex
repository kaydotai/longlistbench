Existing datasets for evaluating document extraction using large language models (LLMs) predominantly focus on key-value pair extraction and fail to address the complexities of long-list entity extraction. However, semi-structured business documents with long incident/line-item lists---such as invoices, purchase orders, and claims summaries---commonly organize data in table-like structures with dozens to hundreds of repetitive entities of the same type, creating a critical gap in current benchmarking efforts. We introduce LongListBench, a synthetic benchmark for long-list entity extraction in semi-structured business documents, inspired by recurring patterns observed in real-world claims documents. Each instance pairs structured ground truth (JSON) with a rendered PDF and an OCR transcript produced by a vision-language model, enabling reproducible evaluation of end-to-end pipelines under layout and transcription noise. We systematically inject seven document phenomena observed in production settings, including page breaks, multi-row entities, exact duplicates, large documents, multiple tables, multi-column layout, and merged cells. Across 80 documents (6{,}828 incident rows), identifier coverage in OCR is near-perfect, while zero-shot LLM baselines (GPT-4o and Claude Sonnet 4) achieve 96.3\% incident-level F1 on five detailed-format documents under a shared prompt. LongListBench targets the measurement problem; algorithmic solutions for robust long-list extraction are left to future work.