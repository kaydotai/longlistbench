Existing document extraction benchmarks focus on short, key-value forms, leaving long-list entity extraction underexplored. Yet business documents such as loss runs, invoices, and itemized bills commonly contain dozens to hundreds of repeated records presented in tables and mixed layouts. We introduce LongListBench, a synthetic benchmark for long-list extraction that pairs structured ground truth (JSON) with rendered PDFs and OCR transcripts, enabling reproducible end-to-end evaluation under layout and transcription noise. The benchmark injects seven document phenomena observed in production---page breaks, multi-row entities, duplicates, large documents, irrelevant tables, multi-column layouts, and merged cells---to stress segmentation and schema-conformant extraction at scale. Across 80 documents (6{,}828 incident rows), incident and reference numbers achieve 100\% verbatim coverage in OCR, while zero-shot LLM baselines achieve 81.9\% (Gemini 2.5) and 78.1\% (GPT-5.2) field-level F1. Results highlight the table format and layout disruptions as primary failure modes, even when identifiers are reliably present.