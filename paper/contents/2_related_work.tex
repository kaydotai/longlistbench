% =============================================================================
% RELATED WORK - Papers to include and why
% =============================================================================

% -----------------------------------------------------------------------------
% 1. DOCUMENT UNDERSTANDING BENCHMARKS
% -----------------------------------------------------------------------------

% VRDU (Wang et al., 2023) - KDD 2023
% WHY: PRIMARY MOTIVATION - explicitly highlights that "models struggle with hierarchical
% fields such as line-items in an invoice." Contains Ad-buy Forms with hierarchical entity
% annotations - the only benchmark addressing repeated/nested fields extraction.
% URL: https://arxiv.org/abs/2211.15421
% Quote: "Extracting hierarchical or repeated entities is really challenging"
% Quote:  models struggle with hierarchical fields such as line-items in an invoice.



% TO BE REVIEWED

% DocVQA (Mathew et al., 2021) - WACV 2021
% WHY: Foundational benchmark for document visual question answering.
% Standard evaluation dataset, but focuses on QA rather than structured extraction.
% URL: https://www.docvqa.org/

% FUNSD (Jaume et al., 2019)
% WHY: Form understanding benchmark with 199 real-world scanned documents.
% Widely used but focuses on key-value pair extraction, not long lists.
% Used to show gap: existing benchmarks don't address our problem.

% CORD (Park et al., 2019) - GOT PUBLISHED AT NEURIPS 2019 - big deal
% WHY: Receipt understanding benchmark. Contains some repeated items but
% limited template diversity and simpler structure than real business documents.

% SROIE (Huang et al., 2019)
% WHY: Receipt information extraction. Similar to CORD - useful baseline
% but doesn't capture complexity of insurance claims or invoices.

% -----------------------------------------------------------------------------
% 2. LAYOUT-AWARE DOCUMENT MODELS (Pre-LLM Era)
% -----------------------------------------------------------------------------

% LayoutLM (Xu et al., 2020) - KDD 2020
% WHY: Pioneering work on multimodal pre-training combining text, layout, and image.
% Established foundation for document understanding but limited by encoder-only architecture.

% LayoutLMv2 (Xu et al., 2021) - ACL 2021
% WHY: Introduced spatial-aware self-attention. SOTA on FUNSD, CORD, SROIE, DocVQA.
% Important baseline showing pre-LLM capabilities.

% LayoutLMv3 (Huang et al., 2022) - ACM MM 2022
% WHY: Unified pre-training for text, layout, and image. F1=0.9029 on FUNSD.
% Best encoder-based model before LLM era.

% GraphLayoutLM (Li et al., 2023) - ACM MM 2023
% WHY: Uses graph structure modeling to capture layout relationships between text nodes.
% Shows importance of structural information beyond positional embeddings.

% -----------------------------------------------------------------------------
% 3. OCR-FREE DOCUMENT UNDERSTANDING
% -----------------------------------------------------------------------------

% Donut (Kim et al., 2022) - ECCV 2022
% WHY: OCR-free end-to-end transformer. Shows alternative to OCR pipeline.
% Important comparison: OCR-based vs OCR-free approaches.
% Limitations: struggles with complex layouts and long documents.
% URL: https://arxiv.org/abs/2111.15664

% -----------------------------------------------------------------------------
% 4. LLM-BASED DOCUMENT EXTRACTION (Most Relevant)
% -----------------------------------------------------------------------------

% LMDX (Perot et al., 2024) - ACL Findings 2024
% WHY: HIGHLY RELEVANT - Google's methodology to adapt LLMs for document extraction.
% Handles singular, repeated, AND hierarchical entities with localization.
% Sets SOTA on VRDU and CORD. Uses chunking for long documents.
% Key innovation: decoding algorithm that discards hallucinations.
% URL: https://arxiv.org/abs/2309.10952

% LayoutLLM (Luo et al., 2024) - CVPR 2024
% WHY: Layout instruction tuning with LLMs. Shows LLMs benefit from layout awareness.
% Achieved 70.82% on DocVQA, 70.96% on FUNSD in zero-shot setting.
% URL: https://openaccess.thecvf.com/content/CVPR2024/papers/Luo_LayoutLLM...

% DocLayLLM (2024)
% WHY: Efficient multimodal extension of LLMs for text-rich document understanding.
% Recent work showing continued advancement in LLM-based approaches.

% ARIAL (2024)
% WHY: Agentic framework for DocVQA with answer localization. SOTA results:
% 88.7 ANLS on DocVQA, 90.0 on FUNSD, 85.5 on CORD, 93.1 on SROIE.
% Shows modular approach with specialized tools.

% -----------------------------------------------------------------------------
% 5. TABLE EXTRACTION
% -----------------------------------------------------------------------------

% Table Transformer / PubTables-1M (Smock et al., 2022) - ICDAR 2023
% WHY: Deep learning for table extraction. GriTS evaluation metric.
% Relevant because tables are structured repeated entities.
% URL: https://github.com/microsoft/table-transformer

% TC-OCR (Anand et al., 2023) - MMIR Workshop 2023
% WHY: End-to-end table recognition pipeline. 25% improvement over Table Transformer.
% Shows table extraction challenges remain unsolved.

% TableFormer (Nassar et al., 2022)
% WHY: Table structure understanding with transformers.
% Relevant for comparison with our structured extraction approach.

% -----------------------------------------------------------------------------
% 6. MULTIMODAL VISION-LANGUAGE MODELS
% -----------------------------------------------------------------------------

% GPT-4V (OpenAI, 2023)
% WHY: We benchmark this. Multimodal capabilities for document understanding.
% Known limitations: hallucinations, errors in complex diagrams.
% URL: https://cdn.openai.com/papers/GPTV_System_Card.pdf

% Claude 3.5 Sonnet (Anthropic, 2024)
% WHY: We benchmark this. Best for data extraction per industry reports.
% Native PDF handling capability.

% Gemini Pro (Google, 2024)
% WHY: We benchmark this. Comparison point for LMDX results.

% MiniGPT-4, LLaVA (2023)
% WHY: Shows emergence of vision-language models. Context for multimodal approaches.

% -----------------------------------------------------------------------------
% 7. SURVEYS (Good for positioning our work)
% -----------------------------------------------------------------------------

% "Deep Learning for Visually Rich Documents" (2024) - IJDAR
% WHY: Comprehensive survey covering 100+ papers. Good for establishing context.
% Covers challenges: text recognition, layout analysis, information fusion.
% URL: https://link.springer.com/article/10.1007/s10032-024-00493-8

% "Survey of Form Understanding in Scanned Documents" (2024) - AI Review
% WHY: Analyzes 15 SOTA models and 10 benchmarks. Transformer models improved
% performance by 25% over traditional methods.
% URL: https://link.springer.com/article/10.1007/s10462-024-11000-0

% -----------------------------------------------------------------------------
% 8. STRUCTURED OUTPUT FROM LLMs
% -----------------------------------------------------------------------------

% Instructor Library / Schema Design Impact
% WHY: Shows field naming impacts performance drastically (4.5% -> 95%).
% Chain-of-thought boosts performance by 60% on GSM8k.
% Relevant for our prompting strategies.

% StructEval (2024)
% WHY: Benchmark for LLMs generating structural outputs (JSON, XML, etc.)
% Directly relevant to our structured extraction task.
% URL: https://arxiv.org/html/2505.20139v1

% -----------------------------------------------------------------------------
% KEY GAPS OUR PAPER ADDRESSES (based on related work):
% -----------------------------------------------------------------------------
% 1. VRDU is the only benchmark with hierarchical entities, but limited scale/diversity
% 2. No benchmark specifically designed for LONG LIST extraction (10+ entities)
% 3. Existing benchmarks don't include true duplicates or multi-row entries
% 4. No standardized evaluation for accumulative/iterative generation approaches
% 5. Insurance claims domain is underrepresented in academic benchmarks
% 6. OCR noise + complex layouts + long lists = unexplored combination
% -----------------------------------------------------------------------------

Research on information extraction (IE) from visually rich documents has produced a broad ecosystem of datasets and models. However, much of the public evaluation landscape emphasizes either short documents (e.g., forms) or key-value extraction, leaving long-list entity extraction underexplored.

\subsection{Document IE benchmarks}
Early and widely used benchmarks such as FUNSD~\cite{jaume2019funsd} focus on form understanding in noisy scans. Receipt datasets and challenges such as SROIE~\cite{huang2021sroie} emphasize OCR and key fields in narrow document types. These benchmarks are valuable, but typically contain relatively short documents and do not directly stress long lists of repeated entities.

DocILE~\cite{simsa2023docile} broadens the scope to business documents and includes line-item recognition, which is closer in spirit to long-list extraction. VRDU~\cite{wang2023vrdu} further argues that hierarchical and repeated fields (e.g., invoice line items) remain difficult for LLM-based extraction. Our benchmark complements these efforts by focusing on list length, repeated entity boundaries, and a targeted taxonomy of long-list failure modes.

\subsection{Document understanding models}
Layout-aware pretraining approaches such as LayoutLM~\cite{xu2020layoutlm} jointly model textual content and 2D document structure, yielding strong performance on a range of document understanding tasks. In parallel, OCR-free approaches such as Donut~\cite{kim2022donut} avoid explicit OCR by directly generating structured outputs from document images, mitigating OCR error propagation at the cost of specialized training.

In contrast, our work is model-agnostic: we provide paired PDF, OCR transcript, and ground truth, enabling evaluation of OCR-based pipelines, OCR-free models, and LLM-based extraction. Our primary goal is to support reproducible measurement of long-list extraction robustness under realistic layout artifacts.
