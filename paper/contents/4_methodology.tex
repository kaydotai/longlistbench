We construct LongListBench, a synthetic benchmark for long-list entity extraction in semi-structured business documents with long incident/line-item lists, inspired by recurring patterns observed in real-world claims documents. Each benchmark instance consists of (i) structured ground truth incidents (JSON), (ii) a rendered PDF, and (iii) an OCR transcript of the PDF in Markdown.

\subsection{Entity schema}
Ground truth incidents follow the schema in \path{benchmarks/models/loss_run.py}. In this paper we focus on extracting seven fields that commonly drive downstream workflows: \texttt{incident\_number}, \texttt{company\_name}, \texttt{date\_of\_loss}, \texttt{status}, \texttt{driver\_name}, \texttt{coverage\_type}, and \texttt{total\_incurred}.

\subsection{Document generation}
Benchmark instances are generated with seeded randomness for reproducibility (\path{benchmarks/generate_claims_benchmark.py}). Structured incidents are created by \path{benchmarks/synthetic/generate_claim_data.py}. We then render the incidents into visually rich documents using \path{benchmarks/synthetic/generate_html.py} and one of two layouts:
\begin{enumerate}
    \item \textbf{Detailed}: a repeated incident block with narrative text and a small financial table.
    \item \textbf{Table}: a compact tabular representation formatted as a CSV-like table.
\end{enumerate}
The HTML is rendered into PDF using headless Chromium via Playwright (\path{benchmarks/synthetic/html_to_pdf.py}).

\subsection{Injected problem types}
We inject seven recurring document phenomena that complicate long-list extraction (Table~\ref{tab:problem-taxonomy}). These effects are applied at the HTML level prior to PDF rendering.

\begin{table}[t]
\centering
\caption{Problem types injected into benchmark documents.}
\label{tab:problem-taxonomy}
\begin{tabular}{p{0.23\linewidth}p{0.71\linewidth}}
\toprule
Problem & Description \\
\midrule
Page breaks & Split incidents/rows across page boundaries. \\
Multi-row entities & Insert line breaks inside cells (e.g., descriptions) to stress OCR and parsing. \\
Exact duplicates & Repeat a subset of incidents verbatim. \\
Large documents & Expand documents to at least 500 incidents to stress context limits. \\
Multiple tables & Add irrelevant tables (e.g., company directory) alongside the true claims. \\
Multi-column layout & Render content in two columns, inducing reading-order ambiguity. \\
Merged cells & Use row/column spanning and omitted repeated cells in table format. \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Dataset scale}
The released dataset (\texttt{longlistbench-v1}, version 1.0.1; see \path{benchmarks/claims/metadata.json}) contains 80 PDFs (40 detailed, 40 table) with 6{,}828 incident rows. Difficulty tiers are configured as 15 easy instances (10 claims/PDF), 12 medium (25 claims/PDF), 8 hard (50 claims/PDF), and 5 extreme (100 claims/PDF nominal). In the extreme tier, enabling \texttt{large\_doc} expands each document to 500 incidents. Enabling \texttt{duplicates} injects additional duplicate rows (up to 5 per document), causing the number of incident rows to exceed the nominal tier size.

Across the 80 documents, the most common injected issues are multi-row entities (62/80), page breaks (56/80), duplicates (56/80), and multiple tables (40/80).
