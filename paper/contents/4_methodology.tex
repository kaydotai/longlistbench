We construct LongListBench, a synthetic benchmark for long-list entity extraction in semi-structured business documents with long incident/line-item lists, inspired by recurring patterns observed in real-world claims documents. Each benchmark instance consists of (i) structured ground truth incidents (JSON), (ii) a rendered PDF, and (iii) an OCR transcript of the PDF in Markdown.

\subsection{Entity schema}
Ground truth incidents follow the schema in \path{benchmarks/models/loss_run.py}, represented as a Pydantic model. The schema includes incident identifiers, policy metadata, narrative text, and nested financial breakdowns (\texttt{bi}, \texttt{pd}, \texttt{lae}, \texttt{ded}). While downstream workflows often emphasize fields such as \texttt{incident\_number}, \texttt{company\_name}, \texttt{date\_of\_loss}, \texttt{status}, \texttt{driver\_name}, \texttt{coverage\_type}, and \texttt{total\_incurred}, our evaluator requires and scores the full schema.

\subsection{Document generation}
Benchmark instances are generated with seeded randomness for reproducibility (\path{benchmarks/generate_claims_benchmark.py}). Structured incidents are created by \path{benchmarks/synthetic/generate_claim_data.py}. We then render the incidents into visually rich documents using \path{benchmarks/synthetic/generate_html.py} and one of two layouts:
\begin{enumerate}
    \item \textbf{Detailed}: a repeated incident block with narrative text and a small financial table.
    \item \textbf{Table}: a compact tabular representation formatted as a CSV-like table.
\end{enumerate}
The HTML is rendered into PDF using headless Chromium via Playwright (\path{benchmarks/synthetic/html_to_pdf.py}).

\begin{figure}[ht]
    \centering
    \IfFileExists{figures/generation_pipeline.png}{
        \includegraphics[width=0.9\linewidth]{figures/generation_pipeline.png}
    }{
        \fbox{\parbox{0.8\textwidth}{\centering\vspace{3cm}Document Generation Pipeline Diagram\vspace{3cm}}}
    }
    \caption{Overview of the benchmark document generation pipeline.}
    \label{fig:generation-pipeline}
\end{figure}

\subsection{Injected problem types}
We inject seven recurring document phenomena that complicate long-list extraction. These effects are applied at the HTML level prior to PDF rendering.

\paragraph{Page breaks.}
In real-world documents, page boundaries frequently split logical entities mid-record. A single incident may begin on one page with identifiers and description, while its financial breakdown appears on the next. This is particularly common in loss runs and itemized bills where dense formatting leaves no natural breakpoints. OCR systems typically process pages independently, producing separate text blocks that must be reassembled. Extraction models must recognize continuation patterns and avoid treating the second half of a split record as a new entity or dropping it entirely.
\begin{figure}[ht]
    \centering
    \IfFileExists{figures/example_page_breaks.png}{
        \includegraphics[width=0.95\linewidth]{figures/example_page_breaks.png}
    }{
        \fbox{\parbox{0.6\textwidth}{\centering\vspace{2cm}Page Breaks Example\vspace{2cm}}}
    }
    \caption{Example of an incident split across a page boundary.}
    \label{fig:example-page-breaks}
\end{figure}

\paragraph{Multi-row entities.}
Table cells often contain text that wraps across multiple lines, especially for description fields, addresses, or claimant lists. When OCR linearizes such content, it may interleave text from adjacent columns or treat each line as a separate cell. For example, a description spanning three lines might appear as three distinct rows in the OCR output, with column alignment lost. Extraction models must recognize that these lines belong to a single logical cell and reconstruct the original cell boundaries from visual or positional cues.
\begin{figure}[ht]
    \centering
    \IfFileExists{figures/example_multi_row.png}{
        \includegraphics[width=0.95\linewidth]{figures/example_multi_row.png}
    }{
        \fbox{\parbox{0.6\textwidth}{\centering\vspace{2cm}Multi-row Entities Example\vspace{2cm}}}
    }
    \caption{Example of a cell containing multiple lines of text.}
    \label{fig:example-multi-row}
\end{figure}

\paragraph{Exact duplicates.}
\mbox{}\\
Production documents sometimes contain intentionally repeated records. In insurance contexts, the same incident may appear multiple times due to amendments, re-openings, or reporting across multiple policy periods. Unlike data entry errors, these duplicates are semantically meaningful and must be preserved in the extracted output. Many extraction pipelines include deduplication as a post-processing step, which would incorrectly collapse valid duplicate entries. Models must faithfully reproduce the document content without applying implicit deduplication logic.

\paragraph{Large documents.}
Real loss runs and itemized bills routinely contain hundreds of line items. A single trucking company's annual loss run may list 200--500 incidents; a hospital bill for a complex procedure can exceed 1{,}000 charge lines. These documents push against context window limits of current LLMs. Even models with 128K+ token windows may exhibit degraded recall on items appearing in the middle of very long documents (the "lost in the middle" phenomenon). We include an extreme tier with 500 incidents per document to measure how extraction quality scales with document length.

\paragraph{Multiple tables.}
Business documents frequently embed auxiliary tables alongside the primary data. A loss run PDF might include a cover page with agent contact information, a summary table of totals by coverage type, or a glossary of status codes---none of which should be extracted as incident records. Models must distinguish the target entity table from these distractors based on schema matching, header recognition, or positional cues. Naive approaches that extract all tabular content will produce false positives from irrelevant tables.

\paragraph{Multi-column layout.}
Some documents use multi-column layouts to fit more content per page. This creates reading-order ambiguity: should text be read left-to-right across columns or top-to-bottom within each column? Standard OCR often assumes a single-column flow, interleaving content from parallel columns into an incoherent sequence. For example, incident \#1's description might be concatenated with incident \#10's financial data if both appear at the same vertical position in adjacent columns. Correct extraction requires column detection and proper reading-order reconstruction.
\begin{figure}[ht]
    \centering
    \IfFileExists{figures/example_multi_column.png}{
        \includegraphics[width=0.95\linewidth]{figures/example_multi_column.png}
    }{
        \fbox{\parbox{0.6\textwidth}{\centering\vspace{2cm}Multi-column Layout Example\vspace{2cm}}}
    }
    \caption{Example of a two-column document layout.}
    \label{fig:example-multi-column}
\end{figure}

\paragraph{Merged cells.}
Tables often use merged cells for visual grouping. A common pattern is merging the company name cell across all incidents belonging to that company, or merging a coverage type header across its associated rows. In the rendered PDF, these appear as single cells spanning multiple rows or columns. OCR may report the merged cell's content only once, leaving subsequent rows with empty values for that field. Extraction models must propagate the merged value to all spanned rows, recognizing that an empty cell indicates inheritance from above rather than a missing value.
\begin{figure}[ht]
    \centering
    \IfFileExists{figures/example_merged_cells.png}{
        \includegraphics[width=0.95\linewidth]{figures/example_merged_cells.png}
    }{
        \fbox{\parbox{0.6\textwidth}{\centering\vspace{2cm}Merged Cells Example\vspace{2cm}}}
    }
    \caption{Example of a table with merged and spanning cells.}
    \label{fig:example-merged-cells}
\end{figure}

\subsection{Dataset scale}
The released dataset (\texttt{longlistbench-v1}, version 1.0.1; see \path{benchmarks/claims/metadata.json}) contains 80 PDFs (40 detailed, 40 table) with 6{,}828 incident rows. Difficulty tiers are configured as 15 easy instances (10 claims/PDF), 12 medium (25 claims/PDF), 8 hard (50 claims/PDF), and 5 extreme (100 claims/PDF nominal). In the extreme tier, enabling \texttt{large\_doc} expands each document to 500 incidents. Enabling \texttt{duplicates} injects additional duplicate rows (up to 5 per document), causing the number of incident rows to exceed the nominal tier size.

Across the 80 documents, the most common injected issues are multi-row entities (62/80), page breaks (56/80), duplicates (56/80), and multiple tables (40/80).
