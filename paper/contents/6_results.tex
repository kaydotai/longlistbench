We summarize results for (i) OCR fidelity and (ii) baseline extraction performance. OCR coverage and LLM baseline numbers are produced by the released scripts in the repository; we additionally report a simple auxiliary identifier-only baseline for context.

\subsection{OCR identifier coverage}
Using \texttt{benchmarks/validate\_ocr\_vs\_golden.py} we measure how often key identifiers from the ground truth appear verbatim in the OCR transcript. Across the full dataset (80 OCR transcripts), incident numbers exhibit 100\% coverage (mean and minimum). Reference numbers exhibit 99.997\% mean coverage with a minimum of 99.8\% (one missing reference in \texttt{extreme\_100\_002\_table}). These results indicate that, for primary identifiers, our OCR step rarely drops information and that most downstream failures are attributable to extraction rather than transcription.

\begin{table}[t]
\centering
\caption{OCR identifier coverage on the full dataset (80 documents).}
\label{tab:ocr-coverage}
\begin{tabular}{lrr}
\toprule
Identifier & Mean coverage & Min coverage \\
\midrule
Incident number & 100.0\% & 100.0\% \\
Reference number & 99.997\% & 99.8\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Zero-shot LLM extraction baseline}
We evaluate three LLMs using the shared prompt and evaluation harness in \texttt{benchmarks/evaluate\_models.py}. The current report (\texttt{benchmarks/results/evaluation\_report.*}) covers five detailed-format samples (two easy, two medium, one hard). GPT-4o and Claude Sonnet 4 achieve the same average incident-level performance: 96.3\% F1, 94.7\% recall, and 98.0\% precision. Average extraction runtime is 10.3 seconds per document for GPT-4o and 25.2 seconds for Claude. Gemini 2.0 Flash failed on all evaluated samples due to API resource exhaustion.

\begin{table}[t]
\centering
\caption{Zero-shot LLM baseline results on five detailed-format samples (\texttt{benchmarks/results/evaluation\_report.json}).}
\label{tab:llm-baselines}
\begin{tabular}{lrrrrr}
\toprule
Model & Samples & Avg Recall & Avg Precision & Avg F1 & Avg Time (s) \\
\midrule
GPT-4o & 5 & 94.7\% & 98.0\% & 96.3\% & 10.3 \\
Claude Sonnet 4 & 5 & 94.7\% & 98.0\% & 96.3\% & 25.2 \\
Gemini 2.0 Flash & 5 & -- & -- & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

Qualitatively, errors often manifest as single-identifier corruption within an otherwise correct long list. For example, in \texttt{easy\_10\_001\_detailed} both GPT-4o and Claude miss one ground-truth incident identifier (\texttt{30010}) and emit one spurious identifier (\texttt{330010}), resulting in 90\% F1.

\subsection{Deterministic identifier extraction baseline}
As a minimal deterministic baseline, we extract incident numbers from OCR text using regular expressions (\texttt{Incident \#\{digits\}} in detailed format and line-initial \texttt{\#\{digits\}} in table format). Scored with the same incident-level metric, this identifier-only baseline achieves 97.4\% average F1 on both detailed and table documents, with 100\% precision and approximately 95\% recall. The residual recall loss is driven primarily by documents with injected exact duplicates: the evaluation metric counts duplicate rows in the ground truth while matching is performed on unique incident identifiers.

These findings suggest that recovering identifiers is largely deterministic under our OCR pipeline, while the main open challenge for long-list extraction is robustly segmenting and populating full per-incident records under layout disruptions (page breaks, multi-column order, irrelevant tables, merged cells) and scale (hundreds of incidents).