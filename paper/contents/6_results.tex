We summarize results for (i) OCR fidelity and (ii) baseline extraction performance. OCR coverage and LLM baseline numbers are produced by the released scripts in the repository.

\subsection{OCR identifier coverage}
Using \texttt{benchmarks/validate\_ocr\_vs\_golden.py} we measure how often key identifiers from the ground truth appear verbatim in the OCR transcript. Across the full dataset (80 OCR transcripts), incident numbers and reference numbers exhibit 100\% coverage (mean and minimum). These results indicate that, for primary identifiers, our OCR step rarely drops information and that most downstream failures are attributable to extraction rather than transcription.

\begin{table}[t]
\centering
\caption{OCR identifier coverage on the full dataset (80 documents).}
\label{tab:ocr-coverage}
\begin{tabular}{lrr}
\toprule
Identifier & Mean coverage & Min coverage \\
\midrule
Incident number & 100.0\% & 100.0\% \\
Reference number & 100.0\% & 100.0\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Zero-shot LLM extraction baseline}
We evaluate three LLMs using the shared prompt and evaluation harness in \texttt{benchmarks/evaluate\_models.py}. The current report (\texttt{benchmarks/results\_medium\_all/evaluation\_report.*}) covers the full medium tier (24 documents: 12 detailed, 12 table; nominally 25 incidents/document) and uses schema-conformant, field-level scoring. Averaged across all medium samples, Gemini 2.5 achieves 80.5\% average F1 (78.5\% recall, 82.6\% precision), GPT-4o achieves 79.7\% average F1 (77.6\% recall, 82.0\% precision), and GPT-5.2 achieves 76.7\% average F1 (74.8\% recall, 78.7\% precision). Across all models, the detailed format is substantially easier than the table format (Table~\ref{tab:llm-baselines-format}).

\begin{table}[t]
\centering
\caption{Zero-shot LLM baseline results on the medium tier (24 documents; nominally 25 incidents/document) under schema-conformant, field-level scoring (\texttt{benchmarks/results\_medium\_all/evaluation\_report.json}).}
\label{tab:llm-baselines}
\begin{tabular}{lrrrr}
\toprule
Model & Samples & Avg Recall & Avg Precision & Avg F1 \\
\midrule
Gemini 2.5 & 24 & 78.5\% & 82.6\% & 80.5\% \\
GPT-4o & 24 & 77.6\% & 82.0\% & 79.7\% \\
GPT-5.2 & 24 & 74.8\% & 78.7\% & 76.7\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Medium-tier baseline F1 by document format (\texttt{benchmarks/results\_medium\_all/evaluation\_report.json}).}
\label{tab:llm-baselines-format}
\begin{tabular}{lrr}
\toprule
Model & Detailed F1 & Table F1 \\
\midrule
Gemini 2.5 & 88.5\% & 72.5\% \\
GPT-4o & 88.9\% & 70.4\% \\
GPT-5.2 & 82.5\% & 70.8\% \\
\bottomrule
\end{tabular}
\end{table}

Qualitatively, errors often manifest as local field-level deviations (e.g., missing optional strings, numeric drift in financial breakdowns, or small identifier formatting mistakes) spread across an otherwise correct long list.

These findings suggest that recovering identifiers is largely deterministic under our OCR pipeline, while the main open challenge for long-list extraction is robustly segmenting and populating full per-incident records under layout disruptions (page breaks, multi-column order, irrelevant tables, merged cells) and scale (hundreds of incidents).