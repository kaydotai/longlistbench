We summarize results for (i) OCR fidelity and (ii) baseline extraction performance. OCR coverage and LLM baseline numbers are produced by the released scripts in the repository.

\subsection{OCR identifier coverage}
Using \path{benchmarks/validate_ocr_vs_golden.py} we measure how often key identifiers from the ground truth appear verbatim in the OCR transcript. Across the full dataset (80 OCR transcripts), incident numbers and reference numbers exhibit 100\% coverage (mean and minimum). These results indicate that, for primary identifiers, our OCR step rarely drops information and that most downstream failures are attributable to extraction rather than transcription.

\begin{table}[t]
\centering
\caption{OCR identifier coverage on the full dataset (80 documents).}
\label{tab:ocr-coverage}
\begin{tabular}{lrr}
\toprule
Identifier & Mean coverage & Min coverage \\
\midrule
Incident number & 100.0\% & 100.0\% \\
Reference number & 100.0\% & 100.0\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Zero-shot LLM extraction baseline}
We evaluate three LLMs using the shared prompt and evaluation harness in \path{benchmarks/evaluate_models.py}. We report schema-conformant, field-level scoring across the full benchmark (80 documents: 40 detailed, 40 table) using the released per-tier evaluation reports (\path{benchmarks/results_*_all/evaluation_report.*}). Averaged across all documents, Gemini 2.5 achieves 81.9\% average F1 (80.4\% recall, 83.4\% precision), GPT-4o achieves 80.0\% average F1 (78.3\% recall, 82.0\% precision), and GPT-5.2 achieves 78.1\% average field-level F1 (76.8\% recall, 79.6\% precision) (Table~\ref{tab:llm-baselines}). Across all models, the detailed format is substantially easier than the table format (Table~\ref{tab:llm-baselines-format}), and performance varies meaningfully across difficulty tiers (Table~\ref{tab:llm-baselines-tier}).

\begin{table}[t]
\centering
\caption{Zero-shot LLM baseline results across the full benchmark (80 documents) under schema-conformant, field-level scoring (computed from \protect\path{benchmarks/results_*_all/evaluation_report.json}).}
\label{tab:llm-baselines}
\begin{tabular}{lrrrr}
\toprule
Model & Samples & Avg Recall & Avg Precision & Avg F1 \\
\midrule
Gemini 2.5 & 80 & 80.4\% & 83.4\% & 81.9\% \\
GPT-4o & 80 & 78.3\% & 82.0\% & 80.0\% \\
GPT-5.2 & 80 & 76.8\% & 79.6\% & 78.1\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Baseline F1 by document format aggregated across all tiers (\protect\path{benchmarks/results_*_all/evaluation_report.json}).}
\label{tab:llm-baselines-format}
\begin{tabular}{lrr}
\toprule
Model & Detailed F1 & Table F1 \\
\midrule
Gemini 2.5 & 89.8\% & 73.9\% \\
GPT-4o & 89.3\% & 70.8\% \\
GPT-5.2 & 83.5\% & 72.8\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Baseline F1 by difficulty tier (average across documents within each tier; \protect\path{benchmarks/results_*_all/evaluation_report.json}).}
\label{tab:llm-baselines-tier}
\begin{tabular}{lrrrr}
\toprule
Tier & Samples & Gemini 2.5 F1 & GPT-4o F1 & GPT-5.2 F1 \\
\midrule
Easy & 30 & 85.1\% & 82.2\% & 80.2\% \\
Medium & 24 & 80.5\% & 79.7\% & 76.7\% \\
Hard & 16 & 78.1\% & 76.6\% & 76.0\% \\
Extreme & 10 & 81.6\% & 80.0\% & 78.9\% \\
\bottomrule
\end{tabular}
\end{table}

Qualitatively, errors often manifest as local field-level deviations (e.g., missing optional strings, numeric drift in financial breakdowns, or small identifier formatting mistakes) spread across an otherwise correct long list.

These findings suggest that recovering identifiers is largely deterministic under our OCR pipeline, while the main open challenge for long-list extraction is robustly segmenting and populating full per-incident records under layout disruptions (page breaks, multi-column order, irrelevant tables, merged cells) and scale (hundreds of incidents).