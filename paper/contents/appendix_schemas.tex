\section*{Appendix A: Evaluation Schemas}
\addcontentsline{toc}{section}{Appendix A: Evaluation Schemas}
\label{sec:appendix-schemas}

A recurring challenge with existing document extraction benchmarks is incomplete or ambiguous schema documentation. Without clear specifications, it can be difficult to understand expected field formats, handling of optional values, or normalization rules. Researchers attempting to reproduce results must often reverse-engineer these details from examples or evaluation scripts, leading to inconsistent implementations and incomparable metrics.

To ensure full reproducibility, we provide complete schemas with explicit type annotations, default values, and field descriptions. The evaluation script validates model outputs against these schemas before scoring, ensuring that format errors are caught early rather than silently degrading metrics.

\subsection*{A.1 Financial Breakdown Schema}

Each incident contains four financial breakdown objects (\texttt{bi}, \texttt{pd}, \texttt{lae}, \texttt{ded}) with the following structure:

\begin{lstlisting}[caption={FinancialBreakdown schema}]
FinancialBreakdown:
    reserve: Float = 0.0        // Amount reserved for potential payout
    paid: Float = 0.0           // Amount already paid
    recovered: Float = 0.0      // Amount recovered (e.g., deductible)
    total_incurred: Float = 0.0 // Reserve + Paid - Recovered
\end{lstlisting}

\subsection*{A.2 Loss Run Incident Schema}

The primary entity schema representing a single insurance claim incident:

\begin{lstlisting}[caption={LossRunIncident schema}]
LossRunIncident:
    // Identifiers
    incident_number: String       // Incident number (e.g., #12345)
    reference_number: String      // Reference ID (e.g., L240123)

    // Company information
    company_name: String          // Trucking company name
    division: String = "General"  // Company division

    // Policy details
    coverage_type: String         // Liability, Physical Damage, etc.
    status: String                // Open or Closed
    policy_number: String         // Policy identifier
    policy_state: String          // Policy state abbreviation

    // Incident details
    cause_code: String?           // Internal cause code (optional)
    description: String           // Detailed incident description
    handler: String = "Claims Adjuster"
    unit_number: String?          // Vehicle/truck unit ID (optional)

    // Dates and locations
    date_of_loss: String          // Date incident occurred
    loss_state: String            // State where loss occurred
    date_reported: String         // Date reported to insurance

    // Parties involved
    agency: String?               // Insurance agency name (optional)
    insured: String               // Insured party name
    claimants: List[String] = []  // List of claimants
    driver_name: String?          // Driver name at time of incident (optional)

    // Financial breakdowns
    bi: FinancialBreakdown        // Bodily Injury
    pd: FinancialBreakdown        // Property Damage
    lae: FinancialBreakdown       // Loss Adjustment Expense
    ded: FinancialBreakdown       // Deductible

    adjuster_notes: String?       // Additional adjuster notes (optional)
\end{lstlisting}

\subsection*{A.3 Extraction Output Schema}

Models are expected to return a JSON object matching the following structure:

\begin{lstlisting}[caption={LossRunExtraction schema}]
LossRunExtraction:
    incidents: List[LossRunIncident]
\end{lstlisting}

\subsection*{A.4 Field Scoring Rules}

During evaluation, fields are normalized and compared as follows:

\begin{itemize}
    \item \textbf{String fields}: Trimmed of whitespace. Optional string fields treat empty strings as \texttt{null}.
    \item \textbf{Numeric fields}: Rounded to two decimal places. Negative zero is normalized to zero.
    \item \textbf{List fields}: Sorted alphabetically for comparison.
    \item \textbf{Financial breakdowns}: Each sub-field (\path{reserve}, \path{paid}, \path{recovered}, \path{total_incurred}) is scored independently.
\end{itemize}

The evaluation computes field-level precision, recall, and F1 by flattening each incident into (\path{incident_id}, \path{field_path}, \path{value}) tuples and comparing predicted tuples against ground truth.
